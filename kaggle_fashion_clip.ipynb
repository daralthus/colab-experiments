{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9pUccqlBh-"
      },
      "source": [
        "# Todo\n",
        "- [x] load clip\n",
        "- [x] load images\n",
        "- [x] batch run inference on images\n",
        "- [x] figure out how to load images from zip\n",
        "- [x] save embeddings\n",
        "- [x] benchmark, gpu: 2h\n",
        "- [x] upload to gcp bucket\n",
        "- [x] embed text as well\n",
        "- [ ] predict a baseline score for kaggle, on image+description cos similarity only"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfH2Q0BIkFry",
        "outputId": "334c21e0-b9a2-4ffe-ceb3-03a7c8bde68c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  9 19:58:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ANzqxPrdZI"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK9UnBnZmmmZ"
      },
      "source": [
        "Don't forget to upload your **kaggle.json** for authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8gReixDle8S"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfKoG--brGPC"
      },
      "source": [
        "1. get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yxJAauDm86J",
        "outputId": "e4ad41d5-822a-42b5-fb34-9f6deacb29ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Downloading h-and-m-personalized-fashion-recommendations.zip to /content\n",
            "100% 28.7G/28.7G [05:28<00:00, 130MB/s] \n",
            "100% 28.7G/28.7G [05:29<00:00, 93.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!KAGGLE_CONFIG_DIR=/content kaggle competitions download -c h-and-m-personalized-fashion-recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clse0OTT0mpo"
      },
      "source": [
        "2. mount zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVEaOacCyXUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7787c0-2496-4afa-f502-b6a59808b218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  libzip4\n",
            "The following NEW packages will be installed:\n",
            "  fuse-zip libzip4\n",
            "0 upgraded, 2 newly installed, 0 to remove and 42 not upgraded.\n",
            "Need to get 65.6 kB of archives.\n",
            "After this operation, 178 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libzip4 amd64 1.1.2-1.1 [37.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 fuse-zip amd64 0.4.4-1 [27.9 kB]\n",
            "Fetched 65.6 kB in 0s (751 kB/s)\n",
            "Selecting previously unselected package libzip4:amd64.\n",
            "(Reading database ... 155203 files and directories currently installed.)\n",
            "Preparing to unpack .../libzip4_1.1.2-1.1_amd64.deb ...\n",
            "Unpacking libzip4:amd64 (1.1.2-1.1) ...\n",
            "Selecting previously unselected package fuse-zip.\n",
            "Preparing to unpack .../fuse-zip_0.4.4-1_amd64.deb ...\n",
            "Unpacking fuse-zip (0.4.4-1) ...\n",
            "Setting up libzip4:amd64 (1.1.2-1.1) ...\n",
            "Setting up fuse-zip (0.4.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y fuse-zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C719KyYAxXcE"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/archive\n",
        "!fuse-zip /content/h-and-m-personalized-fashion-recommendations.zip /content/archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_njuz7q4zhr4"
      },
      "outputs": [],
      "source": [
        "# to unmount\n",
        "# !fusermount -u /content/archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDFoZR1MrIt_"
      },
      "source": [
        "3. get clip model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CNNebBDGRYM",
        "outputId": "854ba2c9-4c55-40b7-aa7a-50c7c96c66c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7evgHLZLGeCt",
        "outputId": "585111cd-bc0b-4703-8d8b-ea8c447d0229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 14.4 MB/s \n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 60.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse, torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9 torch-sparse-0.6.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall -q -y torch-sparse\n",
        "# !pip uninstall -q -y torch-scatter"
      ],
      "metadata": {
        "id": "ZIUGmlH3CDAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg9N13AEq8JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d61ee16-088b-4c4b-e9e3-eecff5df837a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████▏                         | 10 kB 28.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20 kB 37.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ei8l_pcc\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-ei8l_pcc\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.12.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369387 sha256=ce2f349f249d5f7bccf4c4b733d23f19659461d28083cc7ca172b515f7bea754\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-x8j1543z/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Collecting https://github.com/pyg-team/pytorch_geometric/archive/master.zip\n",
            "  Downloading https://github.com/pyg-team/pytorch_geometric/archive/master.zip\n",
            "\u001b[K     / 2.4 MB 3.9 MB/s\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (3.0.8)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric==2.0.5) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric==2.0.5) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric==2.0.5) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric==2.0.5) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.5-py3-none-any.whl size=636898 sha256=5dce419fa7f1c5eccdcae213abc57b9d5611cbbd06302b3010d35b9b95664902\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xs1jn4tf/wheels/70/53/71/38e50390ffab43b7bf5e55f1cdec398bbb09b9b3d2facb4478\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install https://github.com/pyg-team/pytorch_geometric/archive/master.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LliSeZa7yto5"
      },
      "source": [
        "# Get embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3X3L0ZkrUn2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo41qRdisFPK"
      },
      "outputs": [],
      "source": [
        "# TODO to use tpu\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhJdGWOip0W_"
      },
      "outputs": [],
      "source": [
        "# example useage of clip ala. https://github.com/openai/CLIP\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "# image = preprocess(Image.open(\"/content/images/010/0108775015.jpg\")).unsqueeze(0).to(device)\n",
        "# text = clip.tokenize([\"a dress\", \"a dog\", \"a cat\"]).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     image_features = model.encode_image(image)\n",
        "#     text_features = model.encode_text(text)\n",
        "\n",
        "#     logits_per_image, logits_per_text = model(image, text)\n",
        "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "# print(\"Label probs:\", probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEDpSHBc5W71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from os.path import exists\n",
        "\n",
        "\n",
        "class FashionImagesDataset(Dataset):\n",
        "    def __init__(self, transform=lambda id: id):\n",
        "        self.articles = pd.read_csv('/content/archive/articles.csv')\n",
        "        self.articles['img_path'] = self.articles['article_id'].map(lambda id: \"/content/archive/images/0\" + str(id)[0:2] + \"/0\" + str(id) + \".jpg\")\n",
        "        self.valid_idx = self.articles[self.articles.apply(lambda article: exists(article['img_path']), axis=1)]\n",
        "        print('valid and has image:', len(self.valid_idx), 'from:', len(self.articles))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.valid_idx.iloc[idx]['img_path']\n",
        "        image = Image.open(img_path)\n",
        "        label = self.valid_idx.iloc[idx]['article_id']\n",
        "        image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej-lDKY8oSwy",
        "outputId": "7fb6cff5-7213-4adc-fc60-927af89e84e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model_name = 'ViT-B/32'\n",
        "# also ViT-L/14, etc.\n",
        "clip.available_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzA5ARbMTjUs",
        "outputId": "a22ee360-f14e-48f0-d8b5-f77ff7e0ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 195MiB/s]\n"
          ]
        }
      ],
      "source": [
        "model, preprocess = clip.load(model_name, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr5tMoAPu_br"
      },
      "outputs": [],
      "source": [
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hv4yU01t27gh",
        "outputId": "f8c962c4-4d31-45e5-bf9f-b55d2e001a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid and has image: 105100 from: 105542\n"
          ]
        }
      ],
      "source": [
        "dataset = FashionImagesDataset(transform=preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gFqthT8uwAL"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2yd9DQ3OzvI"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBnOFKJHbVJq",
        "outputId": "e98ba432-e19a-4d2a-fa7f-7dde64c7e1ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3, 224, 224]), torch.Size([3, 224, 224]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "images.size(), images.chunk(batch_size)[1].squeeze().size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7NLfRnDmQ8y"
      },
      "source": [
        "# Get image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOlIw9PAvDq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91859d4-3436-4bdc-e46f-280faefa82f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1643/1643 [1:20:46<00:00,  2.95s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "image_features = {}\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(data_loader):\n",
        "      features = model.encode_image(images.to(device))\n",
        "      for label, feature in zip(labels, features):\n",
        "        image_features[label.item()] = feature.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO51X0CIvFCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eefc2e9-9e09-4ac5-a79f-d917cdaeea41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "image_features[111565003].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQVGLfJDtn2M"
      },
      "source": [
        "# Save image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl434UGWfahO"
      },
      "outputs": [],
      "source": [
        "file_name = '/content/fashion-recommendation-image-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj8NalvkROkp"
      },
      "outputs": [],
      "source": [
        "torch.save(image_features, file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tjr65BiVt_g",
        "outputId": "88de7b27-ec90-4617-d937-45ff49616d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "105100"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "len(image_features.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKcXaC-oVoDQ",
        "outputId": "6760e137-546a-468c-d394-bb50d5c6ffcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 130M Apr  5 14:57 /content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -lah $file_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKud3H9UorOw"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPAuaUMZt8r-",
        "outputId": "794ba02f-32f3-4a22-8ba8-1876f6722c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt [Content-Type=application/octet-stream]...\n",
            "|\n",
            "Operation completed over 1 objects/130.0 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp $file_name gs://heii-public/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JXu3G_gd_tC"
      },
      "source": [
        "# Reload image embeddings (e.g.: if you restart colab, etc...)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/fashion-recommendation-image-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ],
      "metadata": {
        "id": "Fi4KCQs_RFiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uB4rFFHD51E-"
      },
      "outputs": [],
      "source": [
        "remote_file = \"https://storage.googleapis.com/heii-public/\" + file_name.replace('/content/', '')\n",
        "remote_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT6eEfSGRHaI"
      },
      "outputs": [],
      "source": [
        "!wget $remote_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPQBHdusR6ZP"
      },
      "outputs": [],
      "source": [
        "image_features = torch.load(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzSXKYloKtUt"
      },
      "source": [
        "# Get text embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "d3FRe7OTKPwK",
        "outputId": "0ec9d256-452c-48a9-c321-d0f98b61d135"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   article_id  product_code          prod_name  product_type_no  \\\n",
              "0   108775015        108775          Strap top              253   \n",
              "1   108775044        108775          Strap top              253   \n",
              "2   108775051        108775      Strap top (1)              253   \n",
              "3   110065001        110065  OP T-shirt (Idro)              306   \n",
              "4   110065002        110065  OP T-shirt (Idro)              306   \n",
              "\n",
              "  product_type_name  product_group_name  graphical_appearance_no  \\\n",
              "0          Vest top  Garment Upper body                  1010016   \n",
              "1          Vest top  Garment Upper body                  1010016   \n",
              "2          Vest top  Garment Upper body                  1010017   \n",
              "3               Bra           Underwear                  1010016   \n",
              "4               Bra           Underwear                  1010016   \n",
              "\n",
              "  graphical_appearance_name  colour_group_code colour_group_name  ...  \\\n",
              "0                     Solid                  9             Black  ...   \n",
              "1                     Solid                 10             White  ...   \n",
              "2                    Stripe                 11         Off White  ...   \n",
              "3                     Solid                  9             Black  ...   \n",
              "4                     Solid                 10             White  ...   \n",
              "\n",
              "   department_name index_code        index_name index_group_no  \\\n",
              "0     Jersey Basic          A        Ladieswear              1   \n",
              "1     Jersey Basic          A        Ladieswear              1   \n",
              "2     Jersey Basic          A        Ladieswear              1   \n",
              "3   Clean Lingerie          B  Lingeries/Tights              1   \n",
              "4   Clean Lingerie          B  Lingeries/Tights              1   \n",
              "\n",
              "   index_group_name section_no            section_name garment_group_no  \\\n",
              "0        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "1        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "2        Ladieswear         16  Womens Everyday Basics             1002   \n",
              "3        Ladieswear         61         Womens Lingerie             1017   \n",
              "4        Ladieswear         61         Womens Lingerie             1017   \n",
              "\n",
              "   garment_group_name                                        detail_desc  \n",
              "0        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "1        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "2        Jersey Basic            Jersey top with narrow shoulder straps.  \n",
              "3   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
              "4   Under-, Nightwear  Microfibre T-shirt bra with underwired, moulde...  \n",
              "\n",
              "[5 rows x 25 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3101af90-2a6f-4104-80cd-78fa7b609a1c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>product_code</th>\n",
              "      <th>prod_name</th>\n",
              "      <th>product_type_no</th>\n",
              "      <th>product_type_name</th>\n",
              "      <th>product_group_name</th>\n",
              "      <th>graphical_appearance_no</th>\n",
              "      <th>graphical_appearance_name</th>\n",
              "      <th>colour_group_code</th>\n",
              "      <th>colour_group_name</th>\n",
              "      <th>...</th>\n",
              "      <th>department_name</th>\n",
              "      <th>index_code</th>\n",
              "      <th>index_name</th>\n",
              "      <th>index_group_no</th>\n",
              "      <th>index_group_name</th>\n",
              "      <th>section_no</th>\n",
              "      <th>section_name</th>\n",
              "      <th>garment_group_no</th>\n",
              "      <th>garment_group_name</th>\n",
              "      <th>detail_desc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>108775015</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>9</td>\n",
              "      <td>Black</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>108775044</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>10</td>\n",
              "      <td>White</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>108775051</td>\n",
              "      <td>108775</td>\n",
              "      <td>Strap top (1)</td>\n",
              "      <td>253</td>\n",
              "      <td>Vest top</td>\n",
              "      <td>Garment Upper body</td>\n",
              "      <td>1010017</td>\n",
              "      <td>Stripe</td>\n",
              "      <td>11</td>\n",
              "      <td>Off White</td>\n",
              "      <td>...</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>A</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>16</td>\n",
              "      <td>Womens Everyday Basics</td>\n",
              "      <td>1002</td>\n",
              "      <td>Jersey Basic</td>\n",
              "      <td>Jersey top with narrow shoulder straps.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110065001</td>\n",
              "      <td>110065</td>\n",
              "      <td>OP T-shirt (Idro)</td>\n",
              "      <td>306</td>\n",
              "      <td>Bra</td>\n",
              "      <td>Underwear</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>9</td>\n",
              "      <td>Black</td>\n",
              "      <td>...</td>\n",
              "      <td>Clean Lingerie</td>\n",
              "      <td>B</td>\n",
              "      <td>Lingeries/Tights</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>61</td>\n",
              "      <td>Womens Lingerie</td>\n",
              "      <td>1017</td>\n",
              "      <td>Under-, Nightwear</td>\n",
              "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110065002</td>\n",
              "      <td>110065</td>\n",
              "      <td>OP T-shirt (Idro)</td>\n",
              "      <td>306</td>\n",
              "      <td>Bra</td>\n",
              "      <td>Underwear</td>\n",
              "      <td>1010016</td>\n",
              "      <td>Solid</td>\n",
              "      <td>10</td>\n",
              "      <td>White</td>\n",
              "      <td>...</td>\n",
              "      <td>Clean Lingerie</td>\n",
              "      <td>B</td>\n",
              "      <td>Lingeries/Tights</td>\n",
              "      <td>1</td>\n",
              "      <td>Ladieswear</td>\n",
              "      <td>61</td>\n",
              "      <td>Womens Lingerie</td>\n",
              "      <td>1017</td>\n",
              "      <td>Under-, Nightwear</td>\n",
              "      <td>Microfibre T-shirt bra with underwired, moulde...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 25 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3101af90-2a6f-4104-80cd-78fa7b609a1c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3101af90-2a6f-4104-80cd-78fa7b609a1c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3101af90-2a6f-4104-80cd-78fa7b609a1c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "articles = pd.read_csv('/content/archive/articles.csv')\n",
        "articles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAzHLLetwZi7"
      },
      "outputs": [],
      "source": [
        "keys = ['derived_name', 'derived_look', 'derived_category', 'prod_name', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'department_name', 'index_name', 'index_group_name', 'section_name', 'garment_group_name', 'detail_desc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjVdt3NxCw2Y",
        "outputId": "1b95170e-fd7c-49af-ddc3-7395f60532a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example derived values:\n",
            "This is a Strap top Vest top\n",
            "It has a Solid Dark Black color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Solid / Black\n",
            "This is a Strap top Vest top\n",
            "It has a Solid Light White color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Solid / White\n",
            "This is a Strap top (1) Vest top\n",
            "It has a Stripe Dusty Light Off White color\n",
            "Ladieswear / Ladieswear / Womens Everyday Basics / Jersey Basic / Jersey Basic / Garment Upper body / Vest top / Stripe / Off White\n"
          ]
        }
      ],
      "source": [
        "articles = pd.read_csv('/content/archive/articles.csv')\n",
        "articles['derived_name'] = articles.apply(lambda row: ' '.join(['This is a', row['prod_name'], row['product_type_name']]), axis=1)\n",
        "articles['derived_look'] = articles.apply(lambda row: ' '.join(['It has a', row['graphical_appearance_name'], row['perceived_colour_value_name'], row['colour_group_name'], 'color']), axis=1)\n",
        "articles['derived_category'] = articles.apply(lambda row: ' / '.join([row['index_group_name'], row['index_name'], row['section_name'], row['department_name'], row['garment_group_name'], row['product_group_name'], row['product_type_name'], row['graphical_appearance_name'], row['colour_group_name']]), axis=1)\n",
        "print('Example derived values:')\n",
        "for i in range(3):\n",
        "    print(articles.iloc[i]['derived_name'])\n",
        "    print(articles.iloc[i]['derived_look'])\n",
        "    print(articles.iloc[i]['derived_category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLkSajvDCJ_l"
      },
      "outputs": [],
      "source": [
        "class FashionTextDataset(Dataset):\n",
        "    def __init__(self, key, articles):\n",
        "        self.key = key\n",
        "        self.articles = articles\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.articles.iloc[idx]['article_id']\n",
        "        # tokenize already pads text\n",
        "        tokens = clip.tokenize(str.strip(str(self.articles.iloc[idx][self.key])), 77, True).squeeze().to(device)\n",
        "        return tokens, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fhyQ-PBd38P"
      },
      "outputs": [],
      "source": [
        "text_dataset = FashionTextDataset(key=keys[0], articles=articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CGUrTyne8eA"
      },
      "outputs": [],
      "source": [
        "text_data_loader = DataLoader(text_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFhL4WW4fCAB"
      },
      "outputs": [],
      "source": [
        "tokens, labels = next(iter(text_data_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwb8oRGlntPt",
        "outputId": "7bb91574-9935-45d6-b74f-c93f6b6dfbaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 15, torch.Size([64, 77]))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "batch_size, len(keys), tokens.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R-IkrZo3t7sX",
        "outputId": "c0461d08-626f-43ce-85a8-d1a53021c4ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|startoftext|>this is a strap top vest top <|endoftext|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tokenizer = clip.simple_tokenizer.SimpleTokenizer()\n",
        "tokenizer.decode(tokens[1].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUA3rK_rDzHQ",
        "outputId": "5766a5c1-0723-4f0d-87e2-81d3ec7ca9e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_name text fields left: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:22<00:00,  8.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_look text fields left: 14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:22<00:00,  8.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  derived_category text fields left: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:30<00:00,  7.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  prod_name text fields left: 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:19<00:00,  8.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  product_type_name text fields left: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  product_group_name text fields left: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:18<00:00,  8.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  graphical_appearance_name text fields left: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  colour_group_name text fields left: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  perceived_colour_value_name text fields left: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:16<00:00,  8.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  department_name text fields left: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  index_name text fields left: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  index_group_name text fields left: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:16<00:00,  8.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  section_name text fields left: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  garment_group_name text fields left: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:17<00:00,  8.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting embeddings for  detail_desc text fields left: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1650/1650 [03:31<00:00,  7.82it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "text_features = {}\n",
        "with torch.no_grad():\n",
        "  for i, key in enumerate(keys):\n",
        "      print('getting embeddings for ', key, 'text fields left:', len(keys)-i)\n",
        "      text_dataset_ = FashionTextDataset(key=key, articles=articles)\n",
        "      text_data_loader_ = DataLoader(text_dataset_, batch_size=64, shuffle=False)\n",
        "      for tokens, labels in tqdm(text_data_loader_):\n",
        "          features = model.encode_text(tokens)\n",
        "          for label, feature in zip(labels, features):\n",
        "              text_features.setdefault(label.item(), {})[key] = feature.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjjHVDoyyT1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c258a6-9bdd-4eca-912d-36295a576b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "text_features[116379047]['derived_name'].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcj1I5gWzERX"
      },
      "source": [
        "# Save text embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xA0S0VzHGAG"
      },
      "source": [
        "should match len(articles):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtAvwcgtzRGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cba38a37-cbe6-4c36-dd4e-df687afa3e17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105542, 105542, True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(articles), len(text_features.keys()), len(articles) == len(text_features.keys()), all([512 == len(text_features[108775015].get(key, {})) for key in keys])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js4mog9QzEAf"
      },
      "outputs": [],
      "source": [
        "text_file_name = '/content/fashion-recommendation-text-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRQ9knT-zLir"
      },
      "outputs": [],
      "source": [
        "torch.save(text_features, text_file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE55_L4DzW5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4c7902-1252-4653-9639-f438d3e62c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt [Content-Type=application/octet-stream]...\n",
            "/ [0 files][    0.0 B/  1.9 GiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "-\n",
            "Operation completed over 1 objects/1.9 GiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp $text_file_name gs://heii-public/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BPuES-KzyWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0094b3da-97c9-4627-ae65-8888d93c2254"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "remote_text_file = \"https://storage.googleapis.com/heii-public/\" + text_file_name.replace('/content/', '')\n",
        "remote_text_file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reload text embeddings"
      ],
      "metadata": {
        "id": "XuJtf_mdYKsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_file_name = '/content/fashion-recommendation-text-embeddings-clip-' + model_name.replace('/', '-') + '.pt'"
      ],
      "metadata": {
        "id": "HRba7tpqYg6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remote_text_file = \"https://storage.googleapis.com/heii-public/\" + text_file_name.replace('/content/', '')"
      ],
      "metadata": {
        "id": "RTxHWAqfYQ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget $remote_text_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdr4KJ7VYQjE",
        "outputId": "5b71d8bc-0435-48e4-9202-eb995387fde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-05 11:33:10--  https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.133.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 480994078 (459M) [application/octet-stream]\n",
            "Saving to: ‘fashion-recommendation-text-embeddings-clip-ViT-B-32.pt’\n",
            "\n",
            "fashion-recommendat 100%[===================>] 458.71M  95.1MB/s    in 5.2s    \n",
            "\n",
            "2022-04-05 11:33:16 (88.5 MB/s) - ‘fashion-recommendation-text-embeddings-clip-ViT-B-32.pt’ saved [480994078/480994078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = torch.load(text_file_name)"
      ],
      "metadata": {
        "id": "Fo7YZMq5ZOTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict customer - product interaction:\n",
        "prerequisites: run setup"
      ],
      "metadata": {
        "id": "LgAiDyoiZYiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import HeteroData, InMemoryDataset, download_url\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "\n",
        "class HMDataset(InMemoryDataset):\n",
        "    image_embeddings_url = \"https://storage.googleapis.com/heii-public/fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\"\n",
        "    text_embeddings_url = \"https://storage.googleapis.com/heii-public/fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\"\n",
        "    raw_dir = \"/content\"\n",
        "    processed_dir = \"/content\"\n",
        "\n",
        "    def __init__(self, root, transform=None, pre_transform=None):\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [\n",
        "            \"fashion-recommendation-image-embeddings-clip-ViT-B-32.pt\",\n",
        "            \"fashion-recommendation-text-embeddings-clip-ViT-B-32.pt\",\n",
        "            \"archive/articles.csv\",\n",
        "            \"archive/customers.csv\",\n",
        "            \"archive/transactions_train.csv\",\n",
        "        ]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return f\"hm_graph.pt\"\n",
        "\n",
        "    def download(self):\n",
        "        download_url(self.image_embeddings_url, self.raw_dir)\n",
        "        download_url(self.text_embeddings_url, self.raw_dir)\n",
        "\n",
        "    def process(self):\n",
        "        self.articles = pd.read_csv(self.raw_paths[2], index_col=\"article_id\")\n",
        "        self.customers = pd.read_csv(self.raw_paths[3], index_col=\"customer_id\").fillna(\n",
        "            0.0\n",
        "        )\n",
        "        self.transactions = pd.read_csv(self.raw_paths[4])\n",
        "\n",
        "        data = HeteroData()\n",
        "\n",
        "        # create node edges\n",
        "        t = self.transactions.to_dict()\n",
        "        customers_id_ix = {v: k for k, v in enumerate(self.customers.index.unique())}\n",
        "        # customers_ix_id = {k: v for k, v in enumerate(self.customers.index.unique())}\n",
        "        articles_id_ix = {v: k for k, v in enumerate(self.articles.index.unique())}\n",
        "        # articles_ix_id = {k: v for k, v in enumerate(self.articles.index.unique())}\n",
        "        src = [customers_id_ix[t[\"customer_id\"][i]] for i in t[\"customer_id\"]]\n",
        "        dst = [articles_id_ix[t[\"article_id\"][i]] for i in t[\"article_id\"]]\n",
        "        data[\"customer\", \"buys\", \"article\"].edge_index = torch.tensor([src, dst]).long()\n",
        "\n",
        "        # avoid out of memory on colab\n",
        "        del t\n",
        "        del customers_id_ix\n",
        "        del articles_id_ix\n",
        "\n",
        "        # encode customers\n",
        "        le = preprocessing.LabelEncoder()\n",
        "        self.customers[\"postal_code\"] = le.fit_transform(self.customers[\"postal_code\"])\n",
        "        self.customers.loc[\n",
        "            self.customers[\"fashion_news_frequency\"] == \"None\", \"fashion_news_frequency\"\n",
        "        ] = 0.0\n",
        "        self.customers.loc[\n",
        "            self.customers[\"fashion_news_frequency\"] == \"NONE\", \"fashion_news_frequency\"\n",
        "        ] = 0.0\n",
        "        customer_features = self.customers[\n",
        "            [\n",
        "                \"postal_code\",\n",
        "                \"age\",\n",
        "                \"fashion_news_frequency\",\n",
        "                \"FN\",\n",
        "                \"Active\",\n",
        "                \"club_member_status\",\n",
        "            ]\n",
        "        ]\n",
        "        customer_features = pd.get_dummies(\n",
        "            customer_features,\n",
        "            columns=[\"age\", \"fashion_news_frequency\", \"club_member_status\"],\n",
        "        )\n",
        "        customer_features = torch.from_numpy(customer_features.to_numpy())\n",
        "\n",
        "        # encode articles\n",
        "        self.article_image_embeddings = torch.load(self.raw_paths[0])\n",
        "        self.article_text_embeddings = torch.load(self.raw_paths[1])\n",
        "        self.articles = self.articles.merge(\n",
        "            self.transactions.groupby(\"article_id\")[\"price\"].mean(),\n",
        "            on=\"article_id\",\n",
        "            how=\"outer\",\n",
        "        ).fillna(0.0)\n",
        "        # self.articles[\"price_bin\"] = pd.qcut(self.articles[\"price\"], 100, labels=False)\n",
        "        self.articles[\"product_type_no\"] = self.articles[\"product_type_no\"].astype(str)\n",
        "        product_type_no_le = preprocessing.LabelEncoder()\n",
        "        self.articles[\"product_type_no\"] = product_type_no_le.fit_transform(\n",
        "            self.articles[\"product_type_no\"]\n",
        "        )\n",
        "        self.articles[\"graphical_appearance_no\"] = self.articles[\n",
        "            \"graphical_appearance_no\"\n",
        "        ].astype(str)\n",
        "        graphical_appearance_no_le = preprocessing.LabelEncoder()\n",
        "        self.articles[\n",
        "            \"graphical_appearance_no\"\n",
        "        ] = graphical_appearance_no_le.fit_transform(\n",
        "            self.articles[\"graphical_appearance_no\"]\n",
        "        )\n",
        "        article_features = self.articles[\n",
        "            [\"product_type_no\", \"graphical_appearance_no\", \"price\"]\n",
        "        ]\n",
        "        # article_features = pd.get_dummies(\n",
        "        #     article_features,\n",
        "        #     columns=[\"price_bin\"],\n",
        "        # )\n",
        "        article_features = torch.from_numpy(article_features.to_numpy())\n",
        "        article_features = torch.cat(\n",
        "            (\n",
        "                article_features,\n",
        "                torch.stack(\n",
        "                    self.articles.apply(\n",
        "                        lambda article: self.article_image_embeddings.get(\n",
        "                            int(article.name), torch.zeros(512)\n",
        "                        ),\n",
        "                        axis=1,\n",
        "                    ).tolist()\n",
        "                ),\n",
        "            ),\n",
        "            1,\n",
        "        )\n",
        "        for key in [\"derived_name\", \"derived_look\", \"derived_category\"]:\n",
        "            article_features = torch.cat(\n",
        "                (\n",
        "                    article_features,\n",
        "                    torch.stack(\n",
        "                        self.articles.apply(\n",
        "                            lambda article: self.article_text_embeddings[\n",
        "                                int(article.name)\n",
        "                            ].get(key, torch.zeros(512)),\n",
        "                            axis=1,\n",
        "                        ).tolist()\n",
        "                    ),\n",
        "                ),\n",
        "                1,\n",
        "            )\n",
        "\n",
        "        # create nodes\n",
        "        data[\"article\"].x = article_features.float()\n",
        "        data[\"customer\"].x = customer_features.float()\n",
        "\n",
        "        # transform?\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        # PyTorch tensor functionality:\n",
        "        # data = data.pin_memory()\n",
        "        # data = data.to('cuda:0', non_blocking=True)\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = HMDataset(\"/content\")"
      ],
      "metadata": {
        "id": "LbKL8eVIJs-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ModuleList\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "import torch_geometric.transforms as T\n",
        "# from hm_dataset import HMDataset\n",
        "from torch_geometric.nn import GCN2Conv, GraphConv, SAGEConv, GATConv, to_hetero\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "dataset = HMDataset(\"/content\")\n",
        "data = dataset[0] # .to(device)\n",
        "data[\"article\"].x = data[\"article\"].x.float()\n",
        "data[\"customer\"].x = data[\"customer\"].x.float()\n",
        "data[(\"customer\", \"buys\", \"article\")].edge_index = data[\n",
        "    (\"customer\", \"buys\", \"article\")\n",
        "].edge_index.long()\n",
        "\n",
        "# Add a reverse ('article', 'rev_buys', 'customer') relation for message passing:\n",
        "data = T.Compose([T.ToUndirected(), T.NormalizeFeatures()])(data).to('cpu')\n",
        "\n",
        "# Perform a link-level split into training, validation, and test edges:\n",
        "train_data, val_data, test_data = T.RandomLinkSplit(\n",
        "    num_val=0.03,\n",
        "    num_test=0.95,\n",
        "    neg_sampling_ratio=0.3,\n",
        "    add_negative_train_samples=True,\n",
        "    edge_types=[(\"customer\", \"buys\", \"article\")],\n",
        "    rev_edge_types=[(\"article\", \"rev_buys\", \"customer\")],\n",
        "    is_undirected=True,\n",
        ")(data)\n",
        "# when neg_sampling_ratio > 0 and add_negative_train_samples=True only then you will have negative edges\n",
        "\n",
        "def create_loader(d, shuffle=True):\n",
        "    return LinkNeighborLoader(\n",
        "      d,\n",
        "      num_neighbors=[-1]*2,\n",
        "      batch_size=1024,\n",
        "      edge_label_index=((\"customer\", \"buys\", \"article\"), d[(\"customer\", \"buys\", \"article\")].edge_label_index),\n",
        "      edge_label=d[(\"customer\", \"buys\", \"article\")].edge_label,\n",
        "      directed=False,\n",
        "      replace=False,\n",
        "      shuffle=shuffle,\n",
        "      pin_memory=True,\n",
        "      num_workers=1\n",
        "    )\n",
        "train_loader = create_loader(train_data)\n",
        "val_loader = create_loader(val_data, False)\n",
        "test_loader = create_loader(test_data)\n",
        "\n",
        "\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, num_layers, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        # self.convs = ModuleList([GATConv((-1, -1), hidden_channels, heads=4, add_self_loops=False, dropout=0.6).jittable() for i in range(self.num_layers-1)])\n",
        "        self.conv = GATConv((-1, -1), hidden_channels, heads=6, add_self_loops=False, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_channels*6, out_channels, heads=1, concat=True, add_self_loops=False, dropout=0.6)\n",
        "        # self.conv3 = SAGEConv((-1, -1), hidden_channels)\n",
        "        self.out_conv = SAGEConv((-1, -1), hidden_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # for layer in self.convs:\n",
        "        #     x = layer(x, edge_index).relu()\n",
        "        x = self.conv(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        # x = self.conv3(x, edge_index).relu()\n",
        "        x = self.out_conv(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = Linear(2 * hidden_channels, hidden_channels)\n",
        "        # self.lin2 = Linear(hidden_channels, hidden_channels)\n",
        "        # self.lin3 = Linear(hidden_channels, hidden_channels)\n",
        "        # self.lin4 = Linear(hidden_channels, hidden_channels)\n",
        "        # self.lin5 = Linear(hidden_channels, hidden_channels)\n",
        "        self.lin6 = Linear(hidden_channels, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = torch.cat([z_dict[\"customer\"][row], z_dict[\"article\"][col]], dim=-1)\n",
        "\n",
        "        z = self.lin1(z).relu()\n",
        "        # z = self.lin2(z).relu()\n",
        "        # z = self.lin3(z).relu()\n",
        "        # z = self.lin4(z).relu()\n",
        "        # z = self.lin5(z).relu()\n",
        "        z = self.lin6(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, num_encoder_layers, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(num_encoder_layers, hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr=\"sum\")\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "\n",
        "model = Model(num_encoder_layers=1, hidden_channels=32).to(device)"
      ],
      "metadata": {
        "id": "zQXjTlyJiG8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjpkZ4CFtxSy",
        "outputId": "1bf66449-061f-40a8-92fa-02d1f95921c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1marticle\u001b[0m={ x=[105542, 2051] },\n",
              "  \u001b[1mcustomer\u001b[0m={ x=[1371980, 95] },\n",
              "  \u001b[1m(customer, buys, article)\u001b[0m={\n",
              "    edge_index=[2, 635768],\n",
              "    edge_label=[826498],\n",
              "    edge_label_index=[2, 826498]\n",
              "  },\n",
              "  \u001b[1m(article, rev_buys, customer)\u001b[0m={ edge_index=[2, 635768] }\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load(f\"/content/link_pred_0.pt\"))"
      ],
      "metadata": {
        "id": "BI4Wk2RS9Vqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024"
      ],
      "metadata": {
        "id": "9BL-eZ8Q9r5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Due to lazy initialization, we need to run one model step so the number\n",
        "# of parameters can be inferred:\n",
        "with torch.no_grad():\n",
        "    batch_ = next(iter(train_loader)).to(device, non_blocking=True)\n",
        "    model.encoder(batch_.x_dict, batch_.edge_index_dict)\n",
        "    del batch_\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.03, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "def train(train_data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(\n",
        "        train_data.x_dict,\n",
        "        train_data.edge_index_dict,\n",
        "        train_data[\"customer\", \"article\"].edge_label_index,\n",
        "    )\n",
        "    target = train_data[\"customer\", \"article\"].edge_label\n",
        "    # loss = F.mse_loss(pred, target)\n",
        "    loss = F.binary_cross_entropy_with_logits(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    pred = model(\n",
        "        data.x_dict,\n",
        "        data.edge_index_dict,\n",
        "        data[\"customer\", \"article\"].edge_label_index,\n",
        "    )\n",
        "    pred = pred.clamp(min=0, max=1)\n",
        "    target = data[\"customer\", \"article\"].edge_label\n",
        "    # loss = F.mse_loss(pred, target).sqrt()\n",
        "    loss = F.binary_cross_entropy_with_logits(pred, target)\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "num_epochs, train_steps, val_steps, test_steps = 301, 2000, 60, 40\n",
        "for epoch in range(1, num_epochs):\n",
        "    train_loss = 0\n",
        "    prog = tqdm(zip(range(train_steps), train_loader), total=train_steps)\n",
        "    for i, batch in prog:\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "        loss = train(batch)\n",
        "        train_loss += loss\n",
        "        # train_rmse = test(batch)\n",
        "        # cuda_mem = torch.cuda.memory_stats()\n",
        "        # cuda_reserved = torch.cuda.max_memory_reserved()\n",
        "        prog.set_description(f\"loss: {loss:.4f}\")\n",
        "    train_loss = train_loss / train_steps\n",
        "\n",
        "    val_loss = 0\n",
        "    for i, batch in tqdm(zip(range(val_steps), val_loader), total=val_steps):\n",
        "        val_loss += test(batch.to(device, non_blocking=True))\n",
        "    val_loss = val_loss / val_steps\n",
        "\n",
        "    test_loss = 0\n",
        "    for i, batch in tqdm(zip(range(test_steps), test_loader), total=test_steps):\n",
        "        test_loss += test(batch.to(device, non_blocking=True))\n",
        "    test_loss = test_loss / test_steps\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        torch.save(model.state_dict(), f\"/content/link_pred_{epoch:03d}.pt\")\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch:03d}, Train: {train_loss:.4f}\"\n",
        "        f\" Val: {val_loss:.4f}, Test: {test_loss:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "Mfs_QnnTqD8J",
        "outputId": "56bad225-09e3-4a16-b92d-1735d0cdea39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.1063:  40%|████      | 808/2000 [03:44<05:30,  3.60it/s]\n",
            "100%|██████████| 60/60 [00:16<00:00,  3.60it/s]\n",
            "100%|██████████| 40/40 [00:27<00:00,  1.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train: 0.0517 Val: 0.4593, Test: 0.5042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss: 0.1042:  40%|████      | 808/2000 [03:43<05:29,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-670bac3096fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_geometric/loader/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0;31m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     \u001b[0;31m# wrong, we set a timeout and if the workers fail to join,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                     \u001b[0;31m# they are killed in the `finally` block.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m                     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMP_STATUS_CHECK_INTERVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1302\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_join_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjqV3Cw8H2zU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), f\"/content/link_pred_num_nei_-1_2_epoch_10.pt\")"
      ],
      "metadata": {
        "id": "83WLWnuYlKjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xF0uRtk0wU8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Link Pred"
      ],
      "metadata": {
        "id": "iiqxFDiRA46h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LinkProp and LinkProp-Multi from the paper:\n",
        "# Revisiting Neighborhood-based Link Prediction for Collaborative Filtering\n",
        "# https://arxiv.org/abs/2203.15789\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_sparse import SparseTensor, matmul\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import ndcg_score, recall_score, precision_score, accuracy_score\n",
        "from itertools import product\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    try:\n",
        "        return torch.load(\"/content/link_prop.pt\")\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "    customers = pd.read_csv(\"/content/archive/customers.csv\")\n",
        "    articles = pd.read_csv(\"/content/archive/articles.csv\")\n",
        "    transactions = pd.read_csv(\"/content/archive/transactions_train.csv\")\n",
        "    customers.reset_index()\n",
        "    articles.reset_index()\n",
        "\n",
        "    src = list(customers[\"customer_id\"])\n",
        "    src_map = {v: k for k, v in enumerate(src)}\n",
        "    dest = list(articles[\"article_id\"])\n",
        "    dest_map = {v: k for k, v in enumerate(dest)}\n",
        "\n",
        "    transactions.reset_index()\n",
        "    transactions[\"src\"] = transactions[\"customer_id\"].map(src_map)\n",
        "    transactions[\"dest\"] = transactions[\"article_id\"].map(dest_map)\n",
        "\n",
        "    # shuffle transactions\n",
        "    edge_index = torch.tensor(\n",
        "        transactions[[\"src\", \"dest\"]].sample(frac=1).values, dtype=torch.long\n",
        "    ).T\n",
        "\n",
        "    # create user-item interaction matrix\n",
        "    M = SparseTensor(\n",
        "        row=edge_index[0],\n",
        "        col=edge_index[1],\n",
        "        value=torch.ones(len(transactions), dtype=torch.float),\n",
        "        sparse_sizes=(len(src), len(dest)),\n",
        "    ).coalesce(\"max\")\n",
        "\n",
        "    torch.save((M, src, dest, src_map, dest_map), \"/content/link_prop.pt\")\n",
        "\n",
        "    return M.to(device), src, dest, src_map, dest_map\n",
        "\n",
        "\n",
        "def split_data(start, count, M):\n",
        "    \"\"\"Selects a range of users\"\"\"\n",
        "    return M[start : start + count, :].coalesce(\"max\")  # .to_dense()\n",
        "\n",
        "\n",
        "def sparse_assign(m, i, j, val):\n",
        "    \"\"\"Sets a value in a sparse matrix at (i,j)\"\"\"\n",
        "    assert i < m.size(0) and j < m.size(1), \"can only assign to existing indices\"\n",
        "    row, col, value = m.coo()\n",
        "    prev = m[i.item(), j.item()].to_dense().squeeze()\n",
        "    return SparseTensor(\n",
        "        row=torch.cat((row.to(device), torch.tensor([i], dtype=torch.long).to(device))),\n",
        "        col=torch.cat((col.to(device), torch.tensor([j], dtype=torch.long).to(device))),\n",
        "        value=torch.cat(\n",
        "            (\n",
        "                value.to(device),\n",
        "                torch.tensor([val], dtype=torch.float).to(device) - prev.to(device),\n",
        "            )\n",
        "        ),\n",
        "        sparse_sizes=(m.size(0), m.size(1)),\n",
        "    ).coalesce(\"add\")\n",
        "\n",
        "\n",
        "def sparse_re_assign_on_mask(m, mask, val, dim=0):\n",
        "    \"\"\"Fills masked values in a sparse matrix with a row or col mask. Only sets values that have been explicitly set before.\"\"\"\n",
        "    row, col, value = m.coo()\n",
        "    prev_row, prev_col, prev_value = m[mask, :].coo() if dim == 0 else m[:, mask].coo()\n",
        "    # restore previous values' indices, assumes mask is boolean and same length as m\n",
        "    if dim == 0:\n",
        "        prev_row = torch.arange(len(mask))[mask][prev_row]\n",
        "    else:\n",
        "        prev_col = torch.arange(len(mask))[mask][prev_col]\n",
        "    return SparseTensor(\n",
        "        row=torch.cat((row.to(device), prev_row.to(device))),\n",
        "        col=torch.cat((col.to(device), prev_col.to(device))),\n",
        "        value=torch.cat((value.to(device), val - prev_value.to(device))),\n",
        "        sparse_sizes=(m.size(0), m.size(1)),\n",
        "    ).coalesce(\"add\")\n",
        "\n",
        "\n",
        "def sparse_cat(m, n):\n",
        "    \"\"\"Stack two sparse matrices\"\"\"\n",
        "    assert m.size(1) == n.size(\n",
        "        1\n",
        "    ), \"cannot stack matrices with different number of columns\"\n",
        "    row, col, value = n.coo()\n",
        "    row = row + m.size(0)\n",
        "    m_row, m_col, m_value = m.coo()\n",
        "    return SparseTensor(\n",
        "        row=torch.cat((m_row.to(device), row.to(device))),\n",
        "        col=torch.cat((m_col.to(device), col.to(device))),\n",
        "        value=torch.cat((m_value.to(device), value.to(device))),\n",
        "        sparse_sizes=(m.size(0) + n.size(0), m.size(1)),\n",
        "    ).coalesce(\"add\")\n",
        "\n",
        "\n",
        "def sparse_batch_op(batch_size, op, m, n):\n",
        "    \"\"\"Batch runs any torch or numpy operation on 2 sparse matrices with same dims\"\"\"\n",
        "    result = SparseTensor(\n",
        "        row=torch.tensor([], dtype=torch.long).to(device),\n",
        "        col=torch.tensor([], dtype=torch.long).to(device),\n",
        "        value=torch.tensor([], dtype=torch.float).to(device),\n",
        "        sparse_sizes=(0, m.size(1)),\n",
        "    )\n",
        "    for i in range(0, m.size(0), batch_size):\n",
        "        end = min(i + batch_size, m.size(0))\n",
        "        res = SparseTensor.from_dense(\n",
        "            op(\n",
        "                m[i:end, :].to_dense().to(device),\n",
        "                n[i:end, :].to_dense().to(device),\n",
        "            )\n",
        "        )\n",
        "        result = sparse_cat(result, res)\n",
        "    return result.to(device)\n",
        "\n",
        "\n",
        "def sparse_nonzero(m):\n",
        "    \"\"\"Returns the nonzero indices of a sparse matrix\"\"\"\n",
        "    row, col, value = m.clone().coo()\n",
        "    return (\n",
        "        row[value != 0].to(device),\n",
        "        col[value != 0].to(device),\n",
        "        value[value != 0].to(device),\n",
        "    )\n",
        "\n",
        "\n",
        "def sparse_blocked_matmul(m, n, block_size=20000):\n",
        "    \"\"\"Dense multiplies a dense and a sparse matrix when you cannot allocate the second matrix all at once\"\"\"\n",
        "    result = torch.empty((m.size(0), 0), dtype=torch.float).to('cpu')\n",
        "    for block_start in range(0, n.size(1), block_size):\n",
        "        block_end = min(block_start + block_size, n.size(1))\n",
        "        res = m.to(device) @ n[:, block_start:block_end].to_dense().to(device)\n",
        "        result = torch.cat((result, res.to('cpu')), dim=1).to('cpu')\n",
        "    return result\n",
        "\n",
        "\n",
        "def intersect2d(a, b):\n",
        "    \"\"\"Returns the intersection of two 2D arrays row by row\"\"\"\n",
        "    return np.array([np.intersect1d(a[i], b[i]) for i in range(len(a))])\n",
        "\n",
        "\n",
        "def sample_user_items(target, ratio=0.4):\n",
        "    \"\"\"Drop some edges for users that have more than 1 item\"\"\"\n",
        "    user_deg = target.sum(dim=1)\n",
        "    candidate = sparse_re_assign_on_mask(target, user_deg > 1, 2, dim=0)\n",
        "    row, col, value = sparse_nonzero(candidate)\n",
        "    rand_mask = (value == 2) & (\n",
        "        torch.rand(len(value)).to(device) < ((value == 2).sum() / len(value) * ratio)\n",
        "    )\n",
        "    value[rand_mask] = 0\n",
        "    value[value == 2] = 1\n",
        "    data = (\n",
        "        SparseTensor(\n",
        "            row=row,\n",
        "            col=col,\n",
        "            value=value,\n",
        "            sparse_sizes=(target.size(0), target.size(1)),\n",
        "        )\n",
        "        .coalesce(\"add\")\n",
        "        .to(device)\n",
        "    )\n",
        "\n",
        "    # take observed out of ground truth\n",
        "    # TODO does this create users with 0 items?\n",
        "    row, col, value = data.clone().coo()\n",
        "    value[value == 1] = 2\n",
        "    value[value == 0] = 1\n",
        "    value[value == 2] = 0\n",
        "    target_new_links = (\n",
        "        SparseTensor(\n",
        "            row=row,\n",
        "            col=col,\n",
        "            value=value,\n",
        "            sparse_sizes=(target.size(0), target.size(1)),\n",
        "        )\n",
        "        .coalesce(\"add\")\n",
        "        .to(device)\n",
        "    )\n",
        "\n",
        "    return data, target_new_links\n",
        "\n",
        "\n",
        "def mean_average_precision(y_true, y_pred, k=12):\n",
        "    \"\"\"Courtesy of https://www.kaggle.com/code/george86/calculate-map-12-fast-faster-fastest\"\"\"\n",
        "    # compute the Rel@K for all items\n",
        "    rel_at_k = np.zeros((len(y_true), k), dtype=int)\n",
        "\n",
        "    # collect the intersection indexes (for the ranking vector) for all pairs\n",
        "    for idx, (truth, pred) in enumerate(zip(y_true, y_pred)):\n",
        "        _, _, inter_idxs = np.intersect1d(\n",
        "            truth, pred[:k], assume_unique=True, return_indices=True\n",
        "        )\n",
        "        rel_at_k[idx, inter_idxs] = 1\n",
        "\n",
        "    # Calculate the intersection counts for all pairs\n",
        "    intersection_count_at_k = rel_at_k.cumsum(axis=1)\n",
        "\n",
        "    # we have the same denominator for all ranking vectors\n",
        "    ranks = np.arange(1, k + 1, 1)\n",
        "\n",
        "    # Calculating the Precision@K for all Ks for all pairs\n",
        "    precisions_at_k = intersection_count_at_k / ranks\n",
        "    # Multiply with the Rel@K for all pairs\n",
        "    precisions_at_k = precisions_at_k * rel_at_k\n",
        "\n",
        "    # Calculate the average precisions @ K for all pairs\n",
        "    average_precisions_at_k = precisions_at_k.mean(axis=1)\n",
        "\n",
        "    # calculate the final MAP@K\n",
        "    map_at_k = average_precisions_at_k.mean()\n",
        "\n",
        "    return map_at_k\n",
        "\n",
        "\n",
        "class LinkPropMulti:\n",
        "    def __init__(self, alpha, beta, gamma, delta, rounds, t, k):\n",
        "        self.rounds, self.k, self.t, self.alpha, self.beta, self.gamma, self.delta = (\n",
        "            rounds,\n",
        "            k,\n",
        "            t,\n",
        "            alpha,\n",
        "            beta,\n",
        "            gamma,\n",
        "            delta,\n",
        "        )\n",
        "        self.user_degrees = None\n",
        "        self.item_degrees = None\n",
        "        self.M = None\n",
        "        self.M_alpha_beta = None\n",
        "        self.M_gamma_delta = None\n",
        "\n",
        "    def set_params(self, alpha, beta, gamma, delta, rounds, t, k):\n",
        "        self.rounds, self.k, self.t, self.alpha, self.beta, self.gamma, self.delta = (\n",
        "            rounds,\n",
        "            k,\n",
        "            t,\n",
        "            alpha,\n",
        "            beta,\n",
        "            gamma,\n",
        "            delta,\n",
        "        )\n",
        "\n",
        "    def fit(self, M):\n",
        "        # reset model\n",
        "        self.M_alpha_beta = None\n",
        "        self.M_gamma_delta = None\n",
        "\n",
        "        # get node degrees\n",
        "        if self.user_degrees == None:\n",
        "            self.user_degrees = M.sum(dim=1)\n",
        "            self.item_degrees = M.sum(dim=0)\n",
        "\n",
        "        # exponentiate degrees by model params\n",
        "        user_alpha = self.user_degrees ** (-self.alpha)\n",
        "        item_beta = self.item_degrees ** (-self.beta)\n",
        "        user_gamma = self.user_degrees ** (-self.gamma)\n",
        "        item_delta = self.item_degrees ** (-self.delta)\n",
        "\n",
        "        # get rid of inf from 1/0\n",
        "        user_alpha[torch.isinf(user_alpha)] = 0.0\n",
        "        item_beta[torch.isinf(item_beta)] = 0.0\n",
        "        user_gamma[torch.isinf(user_gamma)] = 0.0\n",
        "        item_delta[torch.isinf(item_delta)] = 0.0\n",
        "\n",
        "        # to keepe sparsity we can calculate the outer product only for the edges\n",
        "        # so instead of, outer products: alpha_beta = user_alpha.reshape((-1, 1)) * item_beta\n",
        "        # and hadamard product: M_alpha_beta = M * alpha_beta\n",
        "        # we can do:\n",
        "        # row, col, value = sparse_nonzero(M)\n",
        "        # self.M_alpha_beta = SparseTensor(\n",
        "        #     row=row,\n",
        "        #     col=col,\n",
        "        #     value=user_alpha[row] * item_beta[col] * value,\n",
        "        #     sparse_sizes=(M.size(0), M.size(1)),\n",
        "        # )\n",
        "        # self.M_gamma_delta = SparseTensor(\n",
        "        #     row=row,\n",
        "        #     col=col,\n",
        "        #     value=user_gamma[row] * item_delta[col] * value,\n",
        "        #     sparse_sizes=(M.size(0), M.size(1)),\n",
        "        # )\n",
        "        self.user_alpha_sparse = SparseTensor(\n",
        "            row=torch.arange(M.size(0), dtype=torch.long),\n",
        "            col=torch.zeros(M.size(0), dtype=torch.long),\n",
        "            value=user_alpha,\n",
        "            sparse_sizes=(M.size(0), 1),\n",
        "        ).to(device)\n",
        "        self.item_beta_sparse = SparseTensor(\n",
        "            row=torch.zeros(M.size(1), dtype=torch.long),\n",
        "            col=torch.arange(M.size(1), dtype=torch.long),\n",
        "            value=item_beta,\n",
        "            sparse_sizes=(1, M.size(1)),\n",
        "        ).to(device)\n",
        "        self.user_gamma_sparse = SparseTensor(\n",
        "            row=torch.arange(M.size(0), dtype=torch.long),\n",
        "            col=torch.zeros(M.size(0), dtype=torch.long),\n",
        "            value=user_gamma,\n",
        "            sparse_sizes=(M.size(0), 1),\n",
        "        ).to(device)\n",
        "        self.item_delta_sparse = SparseTensor(\n",
        "            row=torch.zeros(M.size(1), dtype=torch.long),\n",
        "            col=torch.arange(M.size(1), dtype=torch.long),\n",
        "            value=item_delta,\n",
        "            sparse_sizes=(1, M.size(1)),\n",
        "        ).to(device)\n",
        "        alpha_beta = matmul(self.user_alpha_sparse, self.item_beta_sparse)\n",
        "        self.M_alpha_beta = sparse_batch_op(2000, torch.multiply, alpha_beta, M)\n",
        "\n",
        "        gamma_delta = matmul(self.user_gamma_sparse, self.item_delta_sparse)\n",
        "        self.M_gamma_delta = sparse_batch_op(2000, torch.multiply, gamma_delta, M)\n",
        "\n",
        "    def fit_multi(self, M, batch_size=600, total=1000):\n",
        "        self.fit(M)\n",
        "        for i in range(self.rounds - 1):\n",
        "            # get top k new links and add them to M\n",
        "            M_new = M.clone()\n",
        "            for start in range(0, total, batch_size):\n",
        "                end = min(start + batch_size, total)\n",
        "                # calculate how many new links we need to add back for the updated node_degrees\n",
        "                number_of_links = M.to_dense().to(device).sum()\n",
        "                k = math.ceil(number_of_links * self.t / (end - start))\n",
        "                # get top k new links\n",
        "                predicted_new_link_indices = self.predict_topk(M, start, end, k)\n",
        "                # add them to M\n",
        "                indices = np.array([])\n",
        "                for i, row in enumerate(predicted_new_link_indices):\n",
        "                    indices = np.concatenate(\n",
        "                        (indices, list(zip([start + i] * len(row), row))), axis=None\n",
        "                    )\n",
        "                indices = indices.reshape(-1, 2)\n",
        "                row, col, value = M_new.coo()\n",
        "                M_new = (\n",
        "                    SparseTensor(\n",
        "                        row=torch.cat(\n",
        "                            (\n",
        "                                row,\n",
        "                                torch.tensor(indices.T[0], dtype=torch.long),\n",
        "                            )\n",
        "                        ),\n",
        "                        col=torch.cat(\n",
        "                            (\n",
        "                                col,\n",
        "                                torch.tensor(indices.T[1], dtype=torch.long),\n",
        "                            )\n",
        "                        ),\n",
        "                        value=torch.cat((value, torch.ones(len(indices)))),\n",
        "                        sparse_sizes=(M.size(0), M.size(1)),\n",
        "                    )\n",
        "                    .coalesce(\"max\")\n",
        "                    .to(device)\n",
        "                )\n",
        "\n",
        "            # recalculate and store node degrees for next round\n",
        "            self.user_degrees = M_new.sum(dim=1)\n",
        "            self.item_degrees = M_new.sum(dim=0)\n",
        "            self.fit(M)\n",
        "\n",
        "    def get_preds_for_users(self, X, start, end, block_size=10000):\n",
        "        # Essentially doing M_alpha_beta.matmul(M.T).matmul(M_gamma_delta),\n",
        "        # just batched and blocked to control memory allocation\n",
        "        assert self.M_alpha_beta is not None, \"You need to fit the model first\"\n",
        "        return matmul(\n",
        "            matmul(self.M_alpha_beta[start:end].to(device), X.t().to(device)).to(\n",
        "                device\n",
        "            ),\n",
        "            self.M_gamma_delta.to(device),\n",
        "        ).to_dense().to(device)\n",
        "\n",
        "        # left = sparse_blocked_matmul(\n",
        "        #     self.M_alpha_beta[start:end].to_dense(), X.t(), block_size\n",
        "        # )\n",
        "        # right = sparse_blocked_matmul(left, self.M_gamma_delta, block_size)\n",
        "        # return right\n",
        "\n",
        "\n",
        "    def predict_topk(self, X, start, end, k):\n",
        "        \"\"\"Return top k new links\"\"\"\n",
        "        # user_pred = self.get_preds_for_users(X, start, end).to_dense().to(device)\n",
        "        user_pred = self.get_preds_for_users(X, start, end).to(device)\n",
        "        # take observed links out of possible predictions\n",
        "        user_pred = (\n",
        "            user_pred - (X[start:end].to_dense().to(device) == 1).float() * 100000\n",
        "        ).clamp(min=0)\n",
        "        user_pred = user_pred.topk(k, dim=1)\n",
        "        # filter out zeros and return indices\n",
        "        return [\n",
        "            user_pred.indices[i, user_pred.values[i] > 0].to(\"cpu\").long().numpy()\n",
        "            for i in range(user_pred.values.size(0))\n",
        "        ]\n",
        "\n",
        "    def predict(self, X, batch_size=600):\n",
        "        user_topk = np.array([], dtype=object)\n",
        "        for start in tqdm(range(0, X.size(0), batch_size)):\n",
        "            end = min(start + batch_size, X.size(0))\n",
        "            user_pred = self.predict_topk(X, start, end, self.k)\n",
        "            user_topk = np.concatenate((user_topk, user_pred), dtype=object)\n",
        "        return user_topk\n",
        "\n",
        "    # def score_ndcg(self, X, y, batch_size=500, total=1000):\n",
        "    #     # TODO ndcg is on the whole item array of 0s and 1s\n",
        "    #     user_topk = np.array([])\n",
        "    #     target_topk = np.array([])\n",
        "    #     for start in tqdm(range(0, total, batch_size)):\n",
        "    #         end = min(start + batch_size, total)\n",
        "    #         user_pred = self.predict_topk(X, start, end, self.k)\n",
        "    #         target_pred = y[start:end].to_dense().squeeze().topk(self.k, dim=1)\n",
        "    #         # filter out zeros\n",
        "    #         target_pred = [\n",
        "    #             target_pred.indices[i, target_pred.values[i] > 0].to(\"cpu\")\n",
        "    #             for i in range(target_pred.values.size(0))\n",
        "    #         ]\n",
        "    #         user_topk = np.concatenate((user_topk, user_pred))\n",
        "    #         target_topk = np.concatenate((target_topk, target_pred))\n",
        "\n",
        "    # return ndcg_score(target_topk, user_topk, k=self.k)\n",
        "\n",
        "    def score(self, X, y, batch_size=600, total=1000):\n",
        "        user_topk = np.array([], dtype=object)\n",
        "        target_topk = np.array([], dtype=object)\n",
        "        scores = []\n",
        "        for start in tqdm(range(0, total, batch_size)):\n",
        "            end = min(start + batch_size, total)\n",
        "            user_pred = self.predict_topk(X, start, end, self.k)\n",
        "            target_pred = y[start:end].to_dense().squeeze().topk(self.k, dim=1)\n",
        "            # filter out zeros\n",
        "            target_pred = [\n",
        "                target_pred.indices[i, target_pred.values[i] > 0]\n",
        "                .to(\"cpu\")\n",
        "                .long()\n",
        "                .numpy()\n",
        "                for i in range(target_pred.values.size(0))\n",
        "            ]\n",
        "            user_topk = np.concatenate((user_topk, user_pred), dtype=object)\n",
        "            target_topk = np.concatenate((target_topk, target_pred), dtype=object)\n",
        "            scores.append(mean_average_precision(target_pred, user_pred, k=self.k))\n",
        "            print(np.array(scores).mean())\n",
        "\n",
        "        return np.array(scores).mean(), user_topk\n",
        "\n",
        "\n",
        "def predict(M, src, dest, alpha, beta, gamma, delta, rounds, t, batch_size=500):\n",
        "    linkProp = LinkPropMulti(\n",
        "        alpha,\n",
        "        beta,\n",
        "        gamma,\n",
        "        delta,\n",
        "        rounds,\n",
        "        t,\n",
        "        k=12,\n",
        "    )\n",
        "    linkProp.fit(M)\n",
        "    preds = linkProp.predict(M, batch_size)\n",
        "    submission = (\n",
        "        pd.DataFrame(\n",
        "            [list(map(lambda x: dest[x], preds[i])) for i in range(len(preds))]\n",
        "        )\n",
        "        .fillna(706016001.0).astype('int')\n",
        "        .astype(\"string\")\n",
        "    )\n",
        "    submission[\"prediction\"] = (\n",
        "        submission[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
        "        .agg(\" 0\".join, axis=1)\n",
        "        .apply(lambda x: \"0\" + x)\n",
        "    )\n",
        "    submission[\"customer_id\"] = src[: len(submission)]\n",
        "    submission[[\"customer_id\", \"prediction\"]].to_csv(\n",
        "        \"/content/submission_lp01.csv\", index=False\n",
        "    )\n",
        "\n",
        "\n",
        "def debug(X, y, alpha, beta, gamma, delta, rounds, t, batch_size=500, total=1000):\n",
        "    linkProp = LinkPropMulti(\n",
        "        alpha,\n",
        "        beta,\n",
        "        gamma,\n",
        "        delta,\n",
        "        rounds,\n",
        "        t,\n",
        "        k=12,\n",
        "    )\n",
        "    linkProp.fit_multi(X, batch_size, total)\n",
        "    score, preds = linkProp.score(X, y, batch_size, total)\n",
        "    print(score)\n",
        "\n",
        "\n",
        "# find optimal params\n",
        "def param_search(M):\n",
        "    param_grid = {\n",
        "        \"alpha\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "        \"beta\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "        \"gamma\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "        \"delta\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
        "    }\n",
        "    linkProp = LinkPropMulti(rounds=1, t=0.05, k=12, alpha=0, beta=0, gamma=0, delta=0)\n",
        "    best = {\"score\": 0}\n",
        "    for params in [dict(zip(param_grid, v)) for v in product(*param_grid.values())]:\n",
        "        val, target = sample_user_items(M, 0.4)\n",
        "        linkProp.set_params(**params, k=12, rounds=1)\n",
        "        linkProp.fit(val)\n",
        "        score = linkProp.score(val, target)\n",
        "        if score > best[\"score\"]:\n",
        "            best[\"score\"] = score\n",
        "            best[\"params\"] = params\n",
        "        print(score, params)\n",
        "\n",
        "    # test\n",
        "    linkProp = LinkPropMulti(rounds=1, t=0.05, k=12, alpha=0, beta=0, gamma=0, delta=0)\n",
        "    test, target = sample_user_items(M, 0.4)\n",
        "    params = best[\"params\"]\n",
        "    linkProp.set_params(**params, k=12, rounds=1)\n",
        "    linkProp.fit(test)\n",
        "    score = linkProp.score(val, target)\n",
        "    print(\"final score\", score, params)\n",
        "\n",
        "\n",
        "# 0.1, 0.9, 0.3, 0.5 > 0.523\n",
        "# 0.3, 0.1, 0.1, 0.3 > 0.547\n",
        "# 0.3, 0.1, 0.3, 0.3 > 0.507\n",
        "# 0.3, 0.1, 0.3, 0.5 > 0.515\n",
        "# 0.3, 0.5, 0.7, 0.3 > 0.509\n",
        "# 0.5, 0.1, 0.9, 0.3 > 0.544\n",
        "# 0.5, 0.5, 0.9, 0.1 > 0.526\n",
        "# 0.5, 0.7, 0.7, 0.3 > 0.519\n"
      ],
      "metadata": {
        "id": "QHSccOsiA8ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M, src, dest, src_map, dest_map = load_data()"
      ],
      "metadata": {
        "id": "UzBQ3kXvEPOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WG1yYDSEQUj",
        "outputId": "52520af3-a242-4ad5-d9de-b4e02b101084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SparseTensor(row=tensor([      0,       0,       0,  ..., 1371978, 1371978, 1371979]),\n",
              "             col=tensor([   99, 16003, 16023,  ..., 95506, 99149, 81594]),\n",
              "             val=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
              "             size=(1371980, 105542), nnz=27306439, density=0.02%)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M, src, dest, src_map, dest_map = load_data()\n",
        "# param_search(M)\n",
        "# predict(M, src, dest, 0.5, 0.1, 0.9, 0.3)\n",
        "data, target = sample_user_items(M, 0.1)\n",
        "# predict(target, src, dest, 0.3, 0.1, 0.1, 0.3)\n",
        "debug(data, target, 0.3, 0.1, 0.1, 0.3, 1, 0.05, batch_size=5000, total=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "id": "q1hDMdwN6t49",
        "outputId": "47df62d2-e0d9-424f-c389-cc50eca0b704"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3106394ec281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_user_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# predict(target, src, dest, 0.3, 0.1, 0.1, 0.3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-dd263afcbc95>\u001b[0m in \u001b[0;36mdebug\u001b[0;34m(X, y, alpha, beta, gamma, delta, rounds, t, batch_size, total)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     )\n\u001b[0;32m--> 496\u001b[0;31m     \u001b[0mlinkProp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinkProp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-dd263afcbc95>\u001b[0m in \u001b[0;36mfit_multi\u001b[0;34m(self, M, batch_size, total)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrounds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# get top k new links and add them to M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-dd263afcbc95>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, M)\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_alpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0msparse_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         ).to(device)\n\u001b[1;32m    303\u001b[0m         self.item_beta_sparse = SparseTensor(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_sparse/tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, row, rowptr, col, value, sparse_sizes, is_sorted, trust_data)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mcsc2csr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mis_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtrust_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_sparse/storage.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, row, rowptr, col, value, sparse_sizes, rowcount, colptr, colcount, csr2csc, csc2csr, is_sorted, trust_data)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict(M[:27400], src, dest, 0.3, 0.1, 0.1, 0.3, 1, 0.05, batch_size=5000)"
      ],
      "metadata": {
        "id": "JJe3v3GZAm2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp /content/submission_lp2.csv gs://heii-public/"
      ],
      "metadata": {
        "id": "UK88hw5JBVfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b98b7657-e150-4d22-b0f8-a656aacf8625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file:///content/submission_lp.csv [Content-Type=text/csv]...\n",
            "/ [0 files][    0.0 B/383.4 MiB]                                                \r==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "|\n",
            "Operation completed over 1 objects/383.4 MiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!KAGGLE_CONFIG_DIR=/content kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f /content/submission_lp2.csv -m \"4\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FgeBu1RPVJm",
        "outputId": "c855f030-75c3-45ee-a468-ac46a523475d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "100% 258M/258M [00:17<00:00, 15.6MB/s]\n",
            "Successfully submitted to H&M Personalized Fashion Recommendations"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('/content/submission_lp2.csv')"
      ],
      "metadata": {
        "id": "SipPuC38Stt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "XZ4xRrlvTeww",
        "outputId": "9a2fd770-9662-4712-abc5-b6128f74b454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         customer_id  \\\n",
              "0  00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...   \n",
              "1  0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...   \n",
              "2  000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...   \n",
              "3  00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...   \n",
              "4  00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...   \n",
              "\n",
              "                                          prediction  \n",
              "0  0804028001 0701108001 0812773001 0557048002 07...  \n",
              "1  0590928022 0601919001 0569248015 0624070002 07...  \n",
              "2  0573284062 0744136002 0561101001 0804454001 06...  \n",
              "3  0710510001 0862631001 0856726001 0589556003 07...  \n",
              "4  0706016001 0706016001 0706016001 0706016001 07...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5845c8e9-12da-4bfc-a671-a95411f2898b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>customer_id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00000dbacae5abe5e23885899a1fa44253a17956c6d1c3...</td>\n",
              "      <td>0804028001 0701108001 0812773001 0557048002 07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000423b00ade91418cceaf3b26c6af3dd342b51fd051e...</td>\n",
              "      <td>0590928022 0601919001 0569248015 0624070002 07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000058a12d5b43e67d225668fa1f8d618c13dc232df0ca...</td>\n",
              "      <td>0573284062 0744136002 0561101001 0804454001 06...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00005ca1c9ed5f5146b52ac8639a40ca9d57aeff4d1bd2...</td>\n",
              "      <td>0710510001 0862631001 0856726001 0589556003 07...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00006413d8573cd20ed7128e53b7b13819fe5cfc2d801f...</td>\n",
              "      <td>0706016001 0706016001 0706016001 0706016001 07...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5845c8e9-12da-4bfc-a671-a95411f2898b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5845c8e9-12da-4bfc-a671-a95411f2898b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5845c8e9-12da-4bfc-a671-a95411f2898b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission[\"prediction\"] = submission['prediction'].apply(lambda x: x.replace(\"tensor(\", \"\").replace(\")\", \"\"))"
      ],
      "metadata": {
        "id": "v7HjO_IITocv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission[[\"customer_id\", \"prediction\"]].to_csv(\"/content/submission_3.csv\", index=False)"
      ],
      "metadata": {
        "id": "kIJ7TZYaUThp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!KAGGLE_CONFIG_DIR=/content kaggle competitions submit -c h-and-m-personalized-fashion-recommendations -f /content/submission_3.csv -m \"3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoltQIRKUYWi",
        "outputId": "d3e6cba3-0e05-494a-d12c-9e457827fc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "100% 258M/258M [00:03<00:00, 81.4MB/s]\n",
            "Successfully submitted to H&M Personalized Fashion Recommendations"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.rand((1371980, 105542))\n",
        "m = torch.rand((1371980, 105542))\n",
        "b = torch.rand((1371980, 105542))"
      ],
      "metadata": {
        "id": "q2zSUvA5Vmva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKatgF3K4elT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "LliSeZa7yto5",
        "X7NLfRnDmQ8y",
        "yQVGLfJDtn2M",
        "5JXu3G_gd_tC",
        "zzSXKYloKtUt",
        "Qcj1I5gWzERX",
        "XuJtf_mdYKsE",
        "LgAiDyoiZYiC"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}